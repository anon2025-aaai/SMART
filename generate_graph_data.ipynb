{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Add parent directory to the system path\n",
    "sys.path.append(\"/mnt/home/network-predictive-analysis/\")\n",
    "import config\n",
    "from io import StringIO\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    curr_path = config.drive_path\n",
    "else:\n",
    "    curr_path = os.getcwd()\n",
    "    \n",
    "data_path = os.path.join(curr_path, \"../\", config.data_path)\n",
    "proc_data_path = os.path.join(curr_path, \"../\", config.processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the link_df, a df version of the dragonfly-link-stats file which holds link information \n",
    "\n",
    "link_file = os.path.join(data_path, \"riodir-61218-1704940703\", \"dragonfly-link-stats\")\n",
    "lines = []\n",
    "\n",
    "with open(link_file, 'r') as file:\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        if (line.startswith('#')):\n",
    "            continue\n",
    "\n",
    "        lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "data_string = '\\n'.join(lines)\n",
    "\n",
    "\n",
    "link_df = pd.read_csv(StringIO(data_string), sep=' ', header=None)\n",
    "\n",
    "link_df.columns = ['source_id', 'src_port_id', 'source_type', 'dest_id', 'dest_type', 'link_type', 'link_traffic', 'link_saturation', 'stalled_chunks']\n",
    "link_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(os.path.join(proc_data_path, \"merged_df_allPorts_scaled.csv\"))\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor X which holds node features over time\n",
    "# X: [num_timesteps, num_nodes, num_features]\n",
    "# note: the 3rd axes (features) contain also the iteration-duration, the target for the prediction\n",
    "\n",
    "X_array = []\n",
    "\n",
    "num_nodes = int(merged_df['router-id'].max()) * 7 + merged_df['port-id'].max() + 1 #nodes of the graph dataset to be built, not computing nodes\n",
    "iterations = merged_df['iteration'].max()\n",
    "\n",
    "node_features_type = ['bw-consumed',\n",
    "                 'downstream-credits-0', 'downstream-credits-1', 'downstream-credits-2', 'downstream-credits-3',\n",
    "                 'vc-occupancy-0', 'vc-occupancy-1', 'vc-occupancy-2', 'vc-occupancy-3']\n",
    "\n",
    "features_aggregation = ['-avg', '-q25', '-q75', '-min', '-max']\n",
    "\n",
    "node_features = ['iteration-duration']\n",
    "node_features.extend([a + b for a, b in product(node_features_type, features_aggregation)])\n",
    "\n",
    "for graph_node_id in range(num_nodes):\n",
    "  filtered_data = merged_df.loc[merged_df['router-id']*7 + merged_df['port-id'] == graph_node_id].reset_index().sort_values(by=\"iteration\")\n",
    "  node_features_over_time = np.transpose(filtered_data[node_features].values)\n",
    "  X_array.append(node_features_over_time)\n",
    "    \n",
    "\n",
    "X = np.array(X_array) #252, 46, 2000\n",
    "X = np.transpose(X, (2, 0, 1)) #2000, 252, 46\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and target tensors have shape:\n",
    "# features: [num_snapshots, num_timesteps_in, num_nodes, num_features]\n",
    "# target: [num_snapshots, num_timesteps_in, num_nodes, num_features]\n",
    "# each snapshot in the features tensor corresponds to a snapshot in the target tensor that we want to predict\n",
    "\n",
    "num_timesteps_in = 50\n",
    "num_timesteps_out = 50\n",
    "\n",
    "indices = [(i, i + num_timesteps_in + num_timesteps_out) for i in range(iterations - num_timesteps_in - num_timesteps_out + 1)]\n",
    "\n",
    "features, target = [], []\n",
    "for lb, ub in indices:\n",
    "  features.append((X[lb : lb + num_timesteps_in, :, :]))\n",
    "  #target.append((X[:, 0, lb + num_timesteps_in : ub]))\n",
    "  target.append((X[lb + num_timesteps_in : ub, :, :]))\n",
    "\n",
    "features = np.array(features)\n",
    "target = np.array(target)\n",
    "features.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test splits for features and targets\n",
    "\n",
    "x_offsets = np.sort(np.concatenate((np.arange(-num_timesteps_in + 1, 1, 1),)))\n",
    "y_offsets = np.sort(np.arange(1, num_timesteps_out + 1, 1))\n",
    "\n",
    "train_ratio = 0.65\n",
    "test_ratio = 0.2\n",
    "# the rest (train_ratio - test_ratio) is used for the validation split\n",
    "\n",
    "num_samples = features.shape[0]\n",
    "num_test = round(num_samples * test_ratio)\n",
    "num_train = round(num_samples * train_ratio)\n",
    "num_val = num_samples - num_test - num_train\n",
    "\n",
    "# train\n",
    "x_train, y_train = features[:num_train], target[:num_train]\n",
    "\n",
    "# val\n",
    "x_val, y_val = (\n",
    "    features[num_train: num_train + num_val],\n",
    "    target[num_train: num_train + num_val],\n",
    ")\n",
    "# test\n",
    "x_test, y_test = features[-num_test:], target[-num_test:]\n",
    "\n",
    "for cat in [\"train\", \"val\", \"test\"]:\n",
    "    _x, _y = locals()[\"x_\" + cat], locals()[\"y_\" + cat]\n",
    "\n",
    "    print(cat, \"x: \", _x.shape, \"y:\", _y.shape)\n",
    "\n",
    "    store_path = os.path.join(proc_data_path, \"SMART\", f\"in{num_timesteps_in}_out{num_timesteps_out}\")\n",
    "    \n",
    "    if not os.path.exists(store_path):\n",
    "        os.makedirs(store_path)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        os.path.join(store_path, \"%s.npz\" % cat),\n",
    "        x=_x,\n",
    "        y=_y,\n",
    "        x_offsets=x_offsets.reshape(list(x_offsets.shape) + [1]),\n",
    "        y_offsets=y_offsets.reshape(list(y_offsets.shape) + [1]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct sparse edge_index tensor\n",
    "# [[n1, n2] [n1, n2] ...]\n",
    "\n",
    "num_routers = int(merged_df['router-id'].max()) + 1\n",
    "num_ports = int(merged_df['port-id'].max()) + 1\n",
    "num_routers_per_group = 4\n",
    "num_ports_per_router = 7\n",
    "\n",
    "edge_index = []\n",
    "\n",
    "\n",
    "# connect all the ports within the same router\n",
    "# (edge_index) has 7*6 * 4 routers * 9 groups = 1512 edges after this step\n",
    "for router_id in range(num_routers):\n",
    "  for port_id_i in range(num_ports):\n",
    "    for port_id_j in range(num_ports):\n",
    "      if port_id_i != port_id_j:\n",
    "        graph_node_id_i = router_id * num_ports_per_router + port_id_i\n",
    "        graph_node_id_j = router_id * num_ports_per_router + port_id_j\n",
    "        edge_index.append([graph_node_id_i, graph_node_id_j])\n",
    "'''\n",
    "  # id of the 1st router in the group the current router_id belongs to\n",
    "  first_router_id = router_id % num_routers_per_group + router_id // num_routers_per_group * num_routers_per_group\n",
    "\n",
    "  if first_router_id % num_routers_per_group == 0:\n",
    "    # id of the 1st node (graph node, corresponding to the router's port) of the 1st router in the first group\n",
    "    first_node_of_first_router_id = first_router_id * num_ports_per_router\n",
    "\n",
    "    # id of the 1st node (graph node) in the current router\n",
    "    first_node_id = router_id * num_ports_per_router\n",
    "\n",
    "    inter_group_connections = [[2, 9], [3, 16], [4, 23], [10, 17], [11, 24], [18, 25]]\n",
    "    for port_id_i, port_id_j in inter_group_connections:\n",
    "      i = first_node_of_first_router_id + port_id_i\n",
    "      j = first_node_of_first_router_id + port_id_j\n",
    "      edge_index.append([i, j])\n",
    "      edge_index.append([j, i])\n",
    "'''\n",
    "\n",
    "# this step extracts the local and global links from the link_df\n",
    "# and results in 6*9*2 (local) + 8*9 (global) edges added to edge_index\n",
    "link_df_filtered = link_df[(link_df['link_type'] == 'L') | (link_df['link_type'] == 'G')]\n",
    "result_df = pd.merge(\n",
    "    link_df_filtered,\n",
    "    link_df_filtered,\n",
    "    left_on=['dest_id', 'source_id', 'link_type'],\n",
    "    right_on=['source_id', 'dest_id', 'link_type'],\n",
    "    suffixes=('_s', '_d')\n",
    ")\n",
    "result_df['source_node_id'] = result_df['source_id_s'] * 7 + result_df['src_port_id_s']\n",
    "result_df['destination_node_id'] = result_df['source_id_d'] * 7 + result_df['src_port_id_d']\n",
    "\n",
    "edge_index.extend(result_df[['destination_node_id', 'source_node_id']].values)\n",
    "\n",
    "len(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "import pickle\n",
    "\n",
    "# Convert sparse edge index to dense adjacency matrix\n",
    "adj_dense = to_dense_adj(torch.tensor(edge_index).T)\n",
    "\n",
    "shape = adj_dense.shape\n",
    "adj_dense = adj_dense.reshape(shape[1], shape[2])\n",
    "\n",
    "# Save to pickle file.\n",
    "adj_matrix_file = os.path.join(proc_data_path, \"adj.pkl\")\n",
    "with open(adj_matrix_file, 'wb') as f:\n",
    "    pickle.dump([range(shape[0]), {i: i for i in range(shape[0])}, adj_dense], f, protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
